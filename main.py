from fastapi import FastAPI, HTTPException, Query
from fastapi.responses import HTMLResponse
from pydantic import BaseModel, Field, validator
from typing import List, Optional, Dict, Union
import asyncio
import aiohttp
from bs4 import BeautifulSoup
import datetime
import json
import logging
from urllib.parse import urlencode, quote
import re
import time

# è¨­å®šæ—¥èªŒè¨˜éŒ„
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å»ºç«‹ FastAPI æ‡‰ç”¨ç¨‹å¼
app = FastAPI(
    title="æ”¿åºœæ¡è³¼ç¶²çˆ¬èŸ² API",
    description="æä¾›æ”¿åºœæ¡è³¼ç¶²è³‡æ–™çˆ¬å–æœå‹™ - ZN Studio è£½ä½œ",
    version="1.4.0",
    contact={
        "name": "Nick Chang",
        "email": "nickleo051216@gmail.com",
        "url": "https://portaly.cc/zn.studio"
    },
    license_info={
        "name": "MIT",
        "url": "https://opensource.org/licenses/MIT"
    }
)

class ScrapeRequest(BaseModel):
    search_terms: List[str] = Field(
        default=["ç³»çµ±", "å¹³å°", "å»ºç½®", "ç¶­é‹"], 
        description="æœå°‹é—œéµå­—åˆ—è¡¨",
        example=["ç³»çµ±", "å¹³å°", "å»ºç½®"]
    )
    start_date: Optional[str] = Field(
        default=None, 
        description="é–‹å§‹æ—¥æœŸ (YYYY/MM/DD æ ¼å¼)",
        example="2024/09/01"
    )
    end_date: Optional[str] = Field(
        default=None, 
        description="çµæŸæ—¥æœŸ (YYYY/MM/DD æ ¼å¼)",
        example="2024/09/24"
    )
    page_size: int = Field(
        default=100, 
        ge=1, 
        le=1000, 
        description="æ¯é ç­†æ•¸ (1-1000)",
        example=100
    )
    tender_type: Optional[str] = Field(
        default=None,
        description="æ‹›æ¨™æ–¹å¼éæ¿¾ (å…¬é–‹æ‹›æ¨™ã€é™åˆ¶æ€§æ‹›æ¨™ç­‰)",
        example="å…¬é–‹æ‹›æ¨™"
    )
    min_budget: Optional[int] = Field(
        default=None,
        description="æœ€ä½é ç®—é‡‘é¡éæ¿¾",
        example=1000000
    )
    agency_filter: Optional[str] = Field(
        default=None,
        description="æ©Ÿé—œåç¨±éæ¿¾é—œéµå­—",
        example="ç’°ä¿ç½²"
    )

    @validator('start_date', 'end_date')
    def validate_date_format(cls, v):
        if v is None:
            return v
        try:
            datetime.datetime.strptime(v, '%Y/%m/%d')
            return v
        except ValueError:
            raise ValueError('æ—¥æœŸæ ¼å¼å¿…é ˆç‚º YYYY/MM/DD')

class ScrapeResponse(BaseModel):
    success: bool = Field(description="æ˜¯å¦æˆåŠŸ")
    data: List[Dict] = Field(description="çˆ¬å–è³‡æ–™")
    count: int = Field(description="è³‡æ–™ç­†æ•¸")
    message: str = Field(description="å›æ‡‰è¨Šæ¯")
    timestamp: str = Field(description="æ™‚é–“æˆ³è¨˜")
    filters_applied: Dict = Field(description="å¥—ç”¨çš„éæ¿¾æ¢ä»¶")

class HealthResponse(BaseModel):
    status: str
    timestamp: str
    service: str
    version: str
    uptime_seconds: Optional[float] = None

class PCCWebScraper:
    """æ”¿åºœæ¡è³¼ç¶²çˆ¬èŸ²é¡åˆ¥ - æ¨™æ¡ˆåç¨±ä¿®æ­£ç‰ˆ"""
    
    def __init__(self):
        self.base_url = "https://web.pcc.gov.tw/prkms/tender/common/basic/readTenderBasic"
        self.session = None
        self.start_time = datetime.datetime.now()
        self.request_count = 0
        
    async def init_session(self):
        if not self.session:
            connector = aiohttp.TCPConnector(
                limit=10, 
                ssl=False,
                limit_per_host=3,
                ttl_dns_cache=300,
                use_dns_cache=True,
            )
            timeout = aiohttp.ClientTimeout(total=60, connect=30)
            self.session = aiohttp.ClientSession(
                connector=connector,
                timeout=timeout,
                headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
                    'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1',
                    'Cache-Control': 'no-cache',
                    'Pragma': 'no-cache'
                }
            )
    
    async def close_session(self):
        if self.session:
            await self.session.close()
            self.session = None
    
    def get_uptime(self) -> float:
        return (datetime.datetime.now() - self.start_time).total_seconds()
    
    async def make_request_with_retry(self, url: str, max_retries: int = 3) -> str:
        """å¸¶é‡è©¦æ©Ÿåˆ¶çš„è«‹æ±‚æ–¹æ³•"""
        for attempt in range(max_retries):
            try:
                self.request_count += 1
                
                if self.request_count > 1:
                    await asyncio.sleep(min(2 + attempt, 5))
                
                async with self.session.get(url) as response:
                    if response.status == 403:
                        logger.warning(f"403 Forbidden - å¯èƒ½éœ€è¦ç™»å…¥æˆ–æ¬Šé™ä¸è¶³")
                        if attempt < max_retries - 1:
                            await asyncio.sleep(10)
                            continue
                        return ""
                    
                    if response.status == 200:
                        html_content = await response.text(encoding='utf-8')
                        
                        if "ç³»çµ±éŒ¯èª¤è¨Šæ¯" in html_content or "ä½¿ç”¨è€…ç„¡æ¬Šé™æ“ä½œæ­¤åŠŸèƒ½" in html_content:
                            logger.warning(f"æ”¶åˆ°éŒ¯èª¤é é¢ï¼Œç‹€æ…‹ç¢¼: {response.status}")
                            if attempt < max_retries - 1:
                                await asyncio.sleep(5 + attempt * 2)
                                continue
                            return ""
                        
                        return html_content
                    else:
                        logger.warning(f"è«‹æ±‚å¤±æ•—ï¼Œç‹€æ…‹ç¢¼: {response.status}")
                        if attempt < max_retries - 1:
                            await asyncio.sleep(3 + attempt)
                            continue
                        return ""
                        
            except Exception as e:
                logger.error(f"è«‹æ±‚ç™¼ç”ŸéŒ¯èª¤ (ç¬¬ {attempt + 1} æ¬¡å˜—è©¦): {str(e)}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(5 + attempt * 2)
                    continue
                return ""
        
        return ""
    
    async def scrape_by_keyword(self, keyword: str, start_date: str, end_date: str, page_size: int = 100) -> List[Dict]:
        if not self.session:
            await self.init_session()
            
        params = {
            'pageSize': page_size,
            'tenderStartDate': start_date,
            'tenderEndDate': end_date,
            'tenderName': keyword,
            'dateType': 'isDate'
        }
        query_string = urlencode(params, quote_via=quote)
        full_url = f"{self.base_url}?{query_string}"
        
        logger.info(f"æ­£åœ¨çˆ¬å–é—œéµå­—: {keyword} | æ—¥æœŸç¯„åœ: {start_date} - {end_date}")
        
        html_content = await self.make_request_with_retry(full_url)
        if not html_content:
            logger.error(f"ç„¡æ³•å–å¾—é—œéµå­— {keyword} çš„è³‡æ–™")
            return []
        
        return self.parse_html_content(html_content, keyword)
    
    def clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡å­—ï¼Œç§»é™¤å¤šé¤˜ç©ºç™½å’Œæ›è¡Œ"""
        if not text:
            return ""
        text = re.sub(r'&[a-zA-Z]+;', ' ', text)
        text = re.sub(r'\s+', ' ', text.strip())
        return text
    
    def improved_parse_tender_info(self, cell_content: str) -> tuple:
        """æ”¹é€²ç‰ˆæ¨™æ¡ˆç·¨è™Ÿå’Œåç¨±è§£æ - ä¿®æ­£æ¨™æ¡ˆåç¨±ç‚ºç©ºçš„å•é¡Œ"""
        if not cell_content:
            return "", ""
        
        # ç§»é™¤HTMLæ¨™ç±¤ï¼Œä¿ç•™æ›è¡Œ
        cell_content = re.sub(r'<[^>]+>', '\n', cell_content)
        
        # åˆ†å‰²è¡Œä¸¦æ¸…ç†ï¼Œä¿ç•™æ‰€æœ‰éç©ºè¡Œ
        lines = [line.strip() for line in cell_content.split('\n') if line.strip()]
        
        if len(lines) == 0:
            return "", ""
        elif len(lines) == 1:
            # åªæœ‰ä¸€è¡Œçš„æƒ…æ³
            single_line = lines[0]
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºç´”ç·¨è™Ÿæ ¼å¼
            if re.match(r'^[A-Za-z0-9\-_]{3,30}$', single_line) and len(single_line) <= 30:
                return single_line, ""
            # æª¢æŸ¥æ˜¯å¦ç‚ºæ›´æ­£å…¬å‘Š
            elif "(æ›´æ­£å…¬å‘Š)" in single_line or "(è®Šæ›´å…¬å‘Š)" in single_line:
                # å˜—è©¦å¾ä¸­æå–ç·¨è™Ÿ
                clean_line = re.sub(r'\([^)]*\)', '', single_line).strip()
                if clean_line and re.match(r'^[A-Za-z0-9\-_]{3,30}$', clean_line):
                    return clean_line, "(æ›´æ­£å…¬å‘Š)"
                else:
                    return "", single_line
            else:
                # æ—¢ä¸æ˜¯ç´”ç·¨è™Ÿä¹Ÿä¸æ˜¯æ›´æ­£å…¬å‘Šï¼Œç•¶ä½œåç¨±
                return "", single_line
        else:
            # å¤šè¡Œæƒ…æ³ - é—œéµä¿®æ­£é»
            first_line = lines[0]
            remaining_lines = lines[1:]
            
            # è™•ç†ç¬¬ä¸€è¡ŒåŒ…å«æ›´æ­£å…¬å‘Šçš„æƒ…æ³
            if "(æ›´æ­£å…¬å‘Š)" in first_line or "(è®Šæ›´å…¬å‘Š)" in first_line:
                clean_first = re.sub(r'\([^)]*\)', '', first_line).strip()
                if clean_first and re.match(r'^[A-Za-z0-9\-_]{3,30}$', clean_first):
                    # ç¬¬ä¸€è¡Œå»æ‰æ›´æ­£å…¬å‘Šå¾Œæ˜¯ç·¨è™Ÿï¼Œå…¶ä»–è¡Œæ˜¯åç¨±
                    if remaining_lines:
                        return clean_first, ' '.join(remaining_lines)
                    else:
                        return clean_first, "(æ›´æ­£å…¬å‘Š)"
                else:
                    # ç¬¬ä¸€è¡Œå»æ‰æ›´æ­£å…¬å‘Šå¾Œä¸æ˜¯ç·¨è™Ÿï¼Œæ•´é«”ç•¶åç¨±
                    return "", ' '.join(lines)
            
            # æª¢æŸ¥ç¬¬ä¸€è¡Œæ˜¯å¦åƒç·¨è™Ÿ
            if (re.match(r'^[A-Za-z0-9\-_]{3,30}$', first_line) and 
                len(first_line) <= 30 and
                not re.search(r'[\u4e00-\u9fff]', first_line)):  # ä¸åŒ…å«ä¸­æ–‡
                
                # ç¬¬ä¸€è¡Œæ˜¯ç·¨è™Ÿï¼Œå…¶ä»–è¡Œæ˜¯åç¨±
                tender_name = ' '.join(remaining_lines) if remaining_lines else ""
                return first_line, tender_name
            else:
                # ç¬¬ä¸€è¡Œä¸åƒç·¨è™Ÿ
                # å˜—è©¦å¾æ‰€æœ‰è¡Œä¸­æ‰¾ç·¨è™Ÿæ ¼å¼
                found_number = ""
                name_parts = []
                
                for line in lines:
                    # æ¸…ç†å¾Œæª¢æŸ¥æ˜¯å¦ç‚ºç·¨è™Ÿ
                    clean_line = re.sub(r'\([^)]*\)', '', line).strip()
                    if (not found_number and 
                        re.match(r'^[A-Za-z0-9\-_]{3,30}$', clean_line) and 
                        len(clean_line) <= 30 and
                        not re.search(r'[\u4e00-\u9fff]', clean_line)):
                        found_number = clean_line
                    else:
                        # ä¸æ˜¯ç·¨è™Ÿçš„è¡ŒåŠ å…¥åç¨±
                        if line.strip() != clean_line:  # æœ‰è¢«æ¸…ç†æ‰å…§å®¹ï¼ˆå¦‚æ›´æ­£å…¬å‘Šï¼‰
                            name_parts.append(line)
                        elif not found_number or line != clean_line:  # ä¸æ˜¯ç·¨è™Ÿæˆ–æœ‰é¡å¤–å…§å®¹
                            name_parts.append(line)
                
                tender_name = ' '.join(name_parts) if name_parts else ""
                return found_number, tender_name
    
    def parse_html_content(self, html: str, keyword: str) -> List[Dict]:
        """ä½¿ç”¨æ”¹é€²ç‰ˆè§£æçš„HTMLå…§å®¹è§£æ"""
        soup = BeautifulSoup(html, 'html.parser')
        results = []
        
        table = soup.find('table', {'id': 'tpam'}) or soup.find('table', {'class': 'tb_01'})
        if not table:
            logger.warning(f"é—œéµå­— {keyword} æœªæ‰¾åˆ°è³‡æ–™è¡¨æ ¼")
            return []
        
        rows = table.find_all('tr')
        today = datetime.date.today().strftime('%Y/%m/%d')
        
        data_rows = [row for row in rows if len(row.find_all('td')) >= 9]
        
        for row in data_rows:
            cols = row.find_all('td')
            if len(cols) < 9:
                continue
                
            try:
                é …æ¬¡ = self.clean_text(cols[0].get_text())
                æ©Ÿé—œåç¨± = self.clean_text(cols[1].get_text())
                
                # ä½¿ç”¨æ”¹é€²ç‰ˆè§£æå‡½æ•¸
                æ¨™æ¡ˆè³‡è¨Š_cell = cols[2]
                æ¨™æ¡ˆè³‡è¨Š_text = æ¨™æ¡ˆè³‡è¨Š_cell.get_text('\n').strip()
                æ¨™æ¡ˆç·¨è™Ÿ, æ¨™æ¡ˆåç¨± = self.improved_parse_tender_info(æ¨™æ¡ˆè³‡è¨Š_text)
                
                å‚³è¼¸æ¬¡æ•¸ = self.clean_text(cols[3].get_text())
                æ‹›æ¨™æ–¹å¼ = self.clean_text(cols[4].get_text())
                æ¡è³¼æ€§è³ª = self.clean_text(cols[5].get_text())
                å…¬å‘Šæ—¥æœŸ = self.clean_text(cols[6].get_text())
                æˆªæ­¢æŠ•æ¨™ = self.clean_text(cols[7].get_text())
                é ç®—é‡‘é¡ = self.clean_text(cols[8].get_text())

                href_url = ""
                a_tag = æ¨™æ¡ˆè³‡è¨Š_cell.find("a")
                if a_tag and a_tag.get('href'):
                    href = a_tag['href']
                    if 'pk=' in href:
                        pk_val = href.split('pk=')[-1].split('&')[0]
                        href_url = f"https://web.pcc.gov.tw/tps/QueryTender/query/searchTenderDetail?pkPmsMain={pk_val}"
                
                é ç®—é‡‘é¡_æ•¸å­— = self.parse_budget_amount(é ç®—é‡‘é¡)
                
                # è³‡æ–™é©—è­‰
                if not æ©Ÿé—œåç¨± or (not æ¨™æ¡ˆç·¨è™Ÿ and not æ¨™æ¡ˆåç¨±):
                    logger.debug(f"è·³éç„¡æ•ˆè³‡æ–™: æ©Ÿé—œ={æ©Ÿé—œåç¨±}, ç·¨è™Ÿ={æ¨™æ¡ˆç·¨è™Ÿ}, åç¨±={æ¨™æ¡ˆåç¨±}")
                    continue
                
                final_data = {
                    "é …æ¬¡": é …æ¬¡,
                    "æ©Ÿé—œåç¨±": æ©Ÿé—œåç¨±,
                    "æ¨™æ¡ˆç·¨è™Ÿ": æ¨™æ¡ˆç·¨è™Ÿ,
                    "æ¨™æ¡ˆåç¨±": æ¨™æ¡ˆåç¨±,
                    "å‚³è¼¸æ¬¡æ•¸": å‚³è¼¸æ¬¡æ•¸,
                    "æ‹›æ¨™æ–¹å¼": æ‹›æ¨™æ–¹å¼,
                    "æ¡è³¼æ€§è³ª": æ¡è³¼æ€§è³ª,
                    "å…¬å‘Šæ—¥æœŸ": å…¬å‘Šæ—¥æœŸ,
                    "æˆªæ­¢æŠ•æ¨™": æˆªæ­¢æŠ•æ¨™,
                    "é ç®—é‡‘é¡": é ç®—é‡‘é¡,
                    "é ç®—é‡‘é¡_æ•¸å­—": é ç®—é‡‘é¡_æ•¸å­—,
                    "ç¶²å€": href_url,
                    "çˆ¬å–æ—¥æœŸ": today,
                    "é—œéµå­—": keyword,
                    # é™¤éŒ¯æ¬„ä½
                    "debug_åŸå§‹æ¨™æ¡ˆè³‡è¨Š": æ¨™æ¡ˆè³‡è¨Š_text[:100] + "..." if len(æ¨™æ¡ˆè³‡è¨Š_text) > 100 else æ¨™æ¡ˆè³‡è¨Š_text
                }

                results.append(final_data)

            except Exception as e:
                logger.error(f"è§£æè¡Œè³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                logger.debug(f"å•é¡Œè¡Œå…§å®¹: {[col.get_text()[:50] for col in cols[:3]]}")
                continue

        logger.info(f"é—œéµå­— {keyword} æˆåŠŸè§£æ {len(results)} ç­†è³‡æ–™")
        return results
    
    def parse_budget_amount(self, budget_str: str) -> Optional[int]:
        """è§£æé ç®—é‡‘é¡å­—ä¸²ç‚ºæ•¸å­—"""
        if not budget_str or budget_str in ["æœªå®š", "ä¾å¥‘ç´„è¾¦ç†", "ä¸å…¬é–‹"]:
            return None
        try:
            clean_str = re.sub(r'[^\d]', '', budget_str)
            if clean_str:
                return int(clean_str)
        except:
            pass
        return None
    
    def apply_filters(self, results: List[Dict], filters: Dict) -> List[Dict]:
        """å¥—ç”¨éæ¿¾æ¢ä»¶"""
        filtered_results = results
        
        if filters.get('tender_type'):
            tender_type = filters['tender_type']
            filtered_results = [r for r in filtered_results if tender_type in r.get('æ‹›æ¨™æ–¹å¼', '')]
        
        if filters.get('min_budget'):
            min_budget = filters['min_budget']
            filtered_results = [r for r in filtered_results if 
                              r.get('é ç®—é‡‘é¡_æ•¸å­—') and r['é ç®—é‡‘é¡_æ•¸å­—'] >= min_budget]
        
        if filters.get('agency_filter'):
            agency_keyword = filters['agency_filter']
            filtered_results = [r for r in filtered_results if 
                              agency_keyword in r.get('æ©Ÿé—œåç¨±', '')]
        
        return filtered_results
    
    async def scrape_multiple_keywords(self, keywords: List[str], start_date: str, end_date: str, 
                                     page_size: int = 100, filters: Dict = None) -> List[Dict]:
        all_results = []
        for i, keyword in enumerate(keywords):
            if i > 0:
                await asyncio.sleep(3)
            
            keyword_results = await self.scrape_by_keyword(keyword, start_date, end_date, page_size)
            all_results.extend(keyword_results)
        
        if filters:
            all_results = self.apply_filters(all_results, filters)
        
        # å»é‡
        seen = set()
        unique_results = []
        for result in all_results:
            identifier = f"{result.get('æ©Ÿé—œåç¨±', '')}|{result.get('æ¨™æ¡ˆç·¨è™Ÿ', '')}|{result.get('æ¨™æ¡ˆåç¨±', '')}"
            if identifier not in seen:
                seen.add(identifier)
                unique_results.append(result)
        
        return unique_results

scraper = PCCWebScraper()

@app.on_event("startup")
async def startup_event():
    await scraper.init_session()
    logger.info("æ”¿åºœæ¡è³¼ç¶²çˆ¬èŸ² API æœå‹™å·²å•Ÿå‹• - æ¨™æ¡ˆåç¨±ä¿®æ­£ç‰ˆ 1.4.0")

@app.on_event("shutdown")
async def shutdown_event():
    await scraper.close_session()
    logger.info("æ”¿åºœæ¡è³¼ç¶²çˆ¬èŸ² API æœå‹™å·²é—œé–‰")

@app.get("/", response_class=HTMLResponse)
async def root():
    """API é¦–é """
    html_content = """
    <!DOCTYPE html>
    <html>
    <head>
        <title>æ”¿åºœæ¡è³¼ç¶²çˆ¬èŸ² API v1.4</title>
        <meta charset="utf-8">
        <style>
            body { font-family: Arial, sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; }
            .header { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
                     color: white; padding: 20px; border-radius: 10px; margin-bottom: 20px; }
            .api-info { background: #f8f9fa; padding: 15px; border-radius: 8px; margin-bottom: 15px; }
            .contact { background: #e3f2fd; padding: 15px; border-radius: 8px; }
            .changelog { background: #fff3e0; padding: 15px; border-radius: 8px; margin-bottom: 15px; }
            a { color: #1976d2; text-decoration: none; }
            a:hover { text-decoration: underline; }
        </style>
    </head>
    <body>
        <div class="header">
            <h1>ğŸš€ æ”¿åºœæ¡è³¼ç¶²çˆ¬èŸ² API v1.4</h1>
            <p>ç‰ˆæœ¬: 1.4.0 | æœå‹™ç‹€æ…‹: é‹è¡Œä¸­ | ä¿®æ­£æ¨™æ¡ˆåç¨±ç‚ºç©ºå•é¡Œ</p>
        </div>
        
        <div class="changelog">
            <h2>ğŸ”§ v1.4.0 é‡è¦ä¿®æ­£</h2>
            <ul>
                <li>âœ… ä¿®æ­£æ¨™æ¡ˆåç¨±å¤§é‡ç‚ºç©ºçš„å•é¡Œ</li>
                <li>âœ… æ”¹é€²å¤šè¡Œå…§å®¹çš„ç·¨è™Ÿåç¨±åˆ†é›¢é‚è¼¯</li>
                <li>âœ… å„ªåŒ–æ›´æ­£å…¬å‘Šçš„è™•ç†æ–¹å¼</li>
                <li>âœ… æå‡æ¨™æ¡ˆè³‡è¨Šè§£æå®Œæ•´æ€§</li>
                <li>âœ… ä¿æŒç·¨è™Ÿè§£æçš„é«˜ç²¾ç¢ºåº¦</li>
            </ul>
        </div>
        
        <div class="api-info">
            <h2>ğŸ“‹ API ç«¯é»</h2>
            <ul>
                <li><strong>GET /docs</strong> - Swagger UI äº’å‹•å¼æ–‡ä»¶</li>
                <li><strong>GET /redoc</strong> - ReDoc æ–‡ä»¶</li>
                <li><strong>GET /health</strong> - å¥åº·ç‹€æ…‹æª¢æŸ¥</li>
                <li><strong>POST /scrape</strong> - éˆæ´»çˆ¬èŸ²æœå°‹ (æ¨è–¦)</li>
                <li><strong>POST /scrape-today</strong> - ä»Šæ—¥å¿«é€Ÿæœå°‹</li>
                <li><strong>POST /test-parse</strong> - æ¸¬è©¦è§£æåŠŸèƒ½</li>
            </ul>
        </div>
        
        <div class="contact">
            <h2>ğŸ‘¨â€ğŸ’» é–‹ç™¼è€…è³‡è¨Š</h2>
            <p><strong>Nick Chang</strong><br>
            ğŸ“§ nickleo051216@gmail.com<br>
            ğŸ“± 0932-684-051<br>
            ğŸŒ <a href="https://portaly.cc/zn.studio">ZN Studio</a><br>
            ğŸ“± <a href="https://www.threads.com/@nickai216">Threads: @nickai216</a><br>
            ğŸ’¬ <a href="https://reurl.cc/1OZNAY">Line ç¤¾ç¾¤</a><br>
            ğŸ¤– <a href="https://lin.ee/Faz0doj">Line OA</a></p>
        </div>
    </body>
    </html>
    """
    return html_content

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """å¥åº·ç‹€æ…‹æª¢æŸ¥"""
    return HealthResponse(
        status="healthy",
        timestamp=datetime.datetime.now().isoformat(),
        service="æ”¿åºœæ¡è³¼ç¶²çˆ¬èŸ²",
        version="1.4.0",
        uptime_seconds=scraper.get_uptime()
    )

@app.post("/scrape", response_model=ScrapeResponse)
async def scrape_tenders(request: ScrapeRequest):
    """
    éˆæ´»çš„æ”¿åºœæ¡è³¼ç¶²çˆ¬èŸ² - v1.4æ¨™æ¡ˆåç¨±ä¿®æ­£ç‰ˆ
    
    ä¸»è¦ä¿®æ­£ï¼š
    - è§£æ±ºæ¨™æ¡ˆåç¨±å¤§é‡ç‚ºç©ºçš„å•é¡Œ
    - æ”¹é€²å¤šè¡Œå…§å®¹çš„è§£æé‚è¼¯
    - å„ªåŒ–ç·¨è™Ÿå’Œåç¨±çš„åˆ†é›¢ç®—æ³•
    """
    if not request.start_date:
        request.start_date = datetime.date.today().strftime('%Y/%m/%d')
    if not request.end_date:
        request.end_date = datetime.date.today().strftime('%Y/%m/%d')
    
    filters = {}
    if request.tender_type:
        filters['tender_type'] = request.tender_type
    if request.min_budget:
        filters['min_budget'] = request.min_budget
    if request.agency_filter:
        filters['agency_filter'] = request.agency_filter
    
    try:
        results = await scraper.scrape_multiple_keywords(
            keywords=request.search_terms,
            start_date=request.start_date,
            end_date=request.end_date,
            page_size=request.page_size,
            filters=filters if filters else None
        )
        
        n8n_format_results = [{"json": item} for item in results]
        
        return ScrapeResponse(
            success=True,
            data=n8n_format_results,
            count=len(results),
            message=f"æˆåŠŸçˆ¬å– {len(results)} ç­†è³‡æ–™ (v1.4.0æ¨™æ¡ˆåç¨±ä¿®æ­£ç‰ˆ)",
            timestamp=datetime.datetime.now().isoformat(),
            filters_applied={
                "keywords": request.search_terms,
                "date_range": f"{request.start_date} ~ {request.end_date}",
                "page_size": request.page_size,
                **filters
            }
        )
        
    except Exception as e:
        logger.error(f"çˆ¬èŸ²åŸ·è¡ŒéŒ¯èª¤: {str(e)}")
        raise HTTPException(status_code=500, detail=f"çˆ¬èŸ²åŸ·è¡Œå¤±æ•—: {str(e)}")

@app.post("/scrape-today")
async def scrape_today(
    keywords: Optional[str] = Query(default="ç’°å¢ƒç›£æ¸¬,åœŸå£¤,åœ°ä¸‹æ°´,ç’°å¢ƒ", description="é—œéµå­—ï¼Œç”¨é€—è™Ÿåˆ†éš”"),
    page_size: Optional[int] = Query(default=100, description="æ¯é ç­†æ•¸")
):
    """ä»Šæ—¥å¿«é€Ÿçˆ¬èŸ² - v1.4ä¿®æ­£ç‰ˆ"""
    today = datetime.date.today().strftime('%Y/%m/%d')
    keyword_list = [k.strip() for k in keywords.split(',')]
    
    try:
        results = await scraper.scrape_multiple_keywords(
            keywords=keyword_list,
            start_date=today,
            end_date=today,
            page_size=page_size
        )
        
        n8n_format_results = [{"json": item} for item in results]
        return n8n_format_results
        
    except Exception as e:
        logger.error(f"ä»Šæ—¥çˆ¬èŸ²åŸ·è¡ŒéŒ¯èª¤: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ä»Šæ—¥çˆ¬èŸ²åŸ·è¡Œå¤±æ•—: {str(e)}")

@app.get("/api-status")
async def api_status():
    """è©³ç´°çš„APIç‹€æ…‹è³‡è¨Š"""
    return {
        "service": "æ”¿åºœæ¡è³¼ç¶²çˆ¬èŸ² API",
        "version": "1.4.0",
        "status": "running",
        "uptime_seconds": scraper.get_uptime(),
        "request_count": scraper.request_count,
        "changelog": {
            "v1.4.0": [
                "ä¿®æ­£æ¨™æ¡ˆåç¨±å¤§é‡ç‚ºç©ºçš„å•é¡Œ",
                "æ”¹é€²å¤šè¡Œå…§å®¹çš„ç·¨è™Ÿåç¨±åˆ†é›¢é‚è¼¯", 
                "å„ªåŒ–æ›´æ­£å…¬å‘Šçš„è™•ç†æ–¹å¼",
                "æå‡æ¨™æ¡ˆè³‡è¨Šè§£æå®Œæ•´æ€§",
                "ä¿æŒç·¨è™Ÿè§£æçš„é«˜ç²¾ç¢ºåº¦"
            ]
        },
        "author": "Nick Changï½œnickleo051216@gmail.comï½œ0932-684-051",
        "contact": {
            "website": "https://portaly.cc/zn.studio",
            "threads": "https://www.threads.com/@nickai216", 
            "line_community": "https://reurl.cc/1OZNAY",
            "line_oa": "https://lin.ee/Faz0doj"
        },
        "timestamp": datetime.datetime.now().isoformat()
    }

@app.post("/test-parse")
async def test_parse_tender_info(content: str):
    """æ¸¬è©¦æ¨™æ¡ˆç·¨è™Ÿåç¨±è§£æåŠŸèƒ½"""
    ç·¨è™Ÿ, åç¨± = scraper.improved_parse_tender_info(content)
    return {
        "åŸå§‹å…§å®¹": content,
        "è§£æçµæœ": {
            "æ¨™æ¡ˆç·¨è™Ÿ": ç·¨è™Ÿ,
            "æ¨™æ¡ˆåç¨±": åç¨±
        },
        "èªªæ˜": "v1.4.0 æ”¹é€²ç‰ˆè§£ææ¸¬è©¦"
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=False, log_level="info")
