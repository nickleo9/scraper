from fastapi import FastAPI, HTTPException, Query
from fastapi.responses import HTMLResponse
from pydantic import BaseModel, Field, validator
from typing import List, Optional, Dict, Union
import asyncio
import aiohttp
from bs4 import BeautifulSoup
import datetime
import json
import logging
from urllib.parse import urlencode, quote

# è¨­å®šæ—¥èªŒè¨˜éŒ„
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å»ºç«‹ FastAPI æ‡‰ç”¨ç¨‹å¼
app = FastAPI(
    title="æ”¿åºœæ¡è³¼ç¶²çˆ¬èŸ² API",
    description="æä¾›æ”¿åºœæ¡è³¼ç¶²è³‡æ–™çˆ¬å–æœå‹™ - ZN Studio è£½ä½œ",
    version="1.1.0",
    contact={
        "name": "Nick Chang",
        "email": "nickleo051216@gmail.com",
        "url": "https://portaly.cc/zn.studio"
    },
    license_info={
        "name": "MIT",
        "url": "https://opensource.org/licenses/MIT"
    }
)

class ScrapeRequest(BaseModel):
    search_terms: List[str] = Field(
        default=["ç³»çµ±", "å¹³å°", "å»ºç½®", "ç¶­é‹"], 
        description="æœå°‹é—œéµå­—åˆ—è¡¨",
        example=["ç³»çµ±", "å¹³å°", "å»ºç½®"]
    )
    start_date: Optional[str] = Field(
        default=None, 
        description="é–‹å§‹æ—¥æœŸ (YYYY/MM/DD æ ¼å¼)",
        example="2024/09/01"
    )
    end_date: Optional[str] = Field(
        default=None, 
        description="çµæŸæ—¥æœŸ (YYYY/MM/DD æ ¼å¼)",
        example="2024/09/24"
    )
    page_size: int = Field(
        default=100, 
        ge=1, 
        le=1000, 
        description="æ¯é ç­†æ•¸ (1-1000)",
        example=100
    )
    tender_type: Optional[str] = Field(
        default=None,
        description="æ‹›æ¨™æ–¹å¼éæ¿¾ (å…¬é–‹æ‹›æ¨™ã€é™åˆ¶æ€§æ‹›æ¨™ç­‰)",
        example="å…¬é–‹æ‹›æ¨™"
    )
    min_budget: Optional[int] = Field(
        default=None,
        description="æœ€ä½é ç®—é‡‘é¡éæ¿¾",
        example=1000000
    )
    agency_filter: Optional[str] = Field(
        default=None,
        description="æ©Ÿé—œåç¨±éæ¿¾é—œéµå­—",
        example="ç’°ä¿ç½²"
    )

    @validator('start_date', 'end_date')
    def validate_date_format(cls, v):
        if v is None:
            return v
        try:
            datetime.datetime.strptime(v, '%Y/%m/%d')
            return v
        except ValueError:
            raise ValueError('æ—¥æœŸæ ¼å¼å¿…é ˆç‚º YYYY/MM/DD')

class ScrapeResponse(BaseModel):
    success: bool = Field(description="æ˜¯å¦æˆåŠŸ")
    data: List[Dict] = Field(description="çˆ¬å–è³‡æ–™")
    count: int = Field(description="è³‡æ–™ç­†æ•¸")
    message: str = Field(description="å›æ‡‰è¨Šæ¯")
    timestamp: str = Field(description="æ™‚é–“æˆ³è¨˜")
    filters_applied: Dict = Field(description="å¥—ç”¨çš„éæ¿¾æ¢ä»¶")

class HealthResponse(BaseModel):
    status: str
    timestamp: str
    service: str
    version: str
    uptime_seconds: Optional[float] = None

class PCCWebScraper:
    """æ”¿åºœæ¡è³¼ç¶²çˆ¬èŸ²é¡åˆ¥ - ä¸éœ€è¦ Selenium çš„ç‰ˆæœ¬"""
    
    def __init__(self):
        self.base_url = "https://web.pcc.gov.tw/prkms/tender/common/basic/readTenderBasic"
        self.session = None
        self.start_time = datetime.datetime.now()
        
    async def init_session(self):
        if not self.session:
            connector = aiohttp.TCPConnector(limit=10, ssl=False)
            timeout = aiohttp.ClientTimeout(total=30)
            self.session = aiohttp.ClientSession(
                connector=connector,
                timeout=timeout,
                headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'Connection': 'keep-alive'
                }
            )
    
    async def close_session(self):
        if self.session:
            await self.session.close()
            self.session = None
    
    def get_uptime(self) -> float:
        return (datetime.datetime.now() - self.start_time).total_seconds()
    
    async def scrape_by_keyword(self, keyword: str, start_date: str, end_date: str, page_size: int = 100) -> List[Dict]:
        if not self.session:
            await self.init_session()
            
        params = {
            'pageSize': page_size,
            'tenderStartDate': start_date,
            'tenderEndDate': end_date,
            'tenderName': keyword,
            'dateType': 'isDate'
        }
        query_string = urlencode(params, quote_via=quote)
        full_url = f"{self.base_url}?{query_string}"
        
        logger.info(f"æ­£åœ¨çˆ¬å–é—œéµå­—: {keyword} | æ—¥æœŸç¯„åœ: {start_date} - {end_date}")
        
        try:
            async with self.session.get(full_url) as response:
                if response.status != 200:
                    logger.error(f"è«‹æ±‚å¤±æ•—: {response.status}")
                    return []
                html_content = await response.text()
                return self.parse_html_content(html_content, keyword)
        except Exception as e:
            logger.error(f"çˆ¬å–é—œéµå­— {keyword} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return []
    
    def parse_html_content(self, html: str, keyword: str) -> List[Dict]:
        """è§£æç¶²é å…§å®¹ï¼Œåˆ†é–‹æ¡ˆè™Ÿè·Ÿåç¨±"""
        soup = BeautifulSoup(html, 'html.parser')
        results = []
        table = soup.find('table', {'id': 'tpam'}) or soup.find('table', {'class': 'tb_01'})
        if not table:
            logger.warning(f"é—œéµå­— {keyword} æœªæ‰¾åˆ°è³‡æ–™è¡¨æ ¼")
            return []
        
        rows = table.find_all('tr')
        today = datetime.date.today().strftime('%Y/%m/%d')
        
        for row in rows[1:]:
            cols = row.find_all('td')
            if len(cols) < 9:
                continue
            try:
                # å»ºç«‹åŸºæœ¬è³‡æ–™å­—å…¸
                row_data = [col.get_text("\n", strip=True) for col in cols[:9]]
                keys = ["é …æ¬¡", "æ©Ÿé—œåç¨±", "æ¨™æ¡ˆæ¡ˆè™Ÿ&ç·¨è™Ÿåç¨±", "å‚³è¼¸æ¬¡æ•¸", "æ‹›æ¨™æ–¹å¼", "æ¡è³¼æ€§è³ª", "å…¬å‘Šæ—¥æœŸ", "æˆªæ­¢æŠ•æ¨™", "é ç®—é‡‘é¡"]
                row_dict = dict(zip(keys, row_data))

                # å–å¾—é€£çµ
                href_url = ""
                a_tag = cols[2].find("a")
                if a_tag and a_tag.get('href') and 'pk=' in a_tag.get('href'):
                    pk_val = a_tag['href'].split('pk=')[-1].split('&')[0]
                    href_url = f"https://web.pcc.gov.tw/tps/QueryTender/query/searchTenderDetail?pkPmsMain={pk_val}"
                
                # åˆ†é–‹æ¡ˆè™Ÿèˆ‡åç¨±
                raw = row_dict.pop("æ¨™æ¡ˆæ¡ˆè™Ÿ&ç·¨è™Ÿåç¨±", "")
                if "\n" in raw:
                   line1, line2 = raw.split("\n", 1)
                   ç·¨è™Ÿ = line1.split()[0] if line1.split() else ""
                   åç¨± = line2.strip()
                else:
                   ç·¨è™Ÿ = ""
                   åç¨± = raw
                
                # è™•ç†é ç®—é‡‘é¡ï¼Œè½‰æ›ç‚ºæ•¸å­—ä¾¿æ–¼éæ¿¾
                é ç®—é‡‘é¡_åŸå§‹ = row_dict.get("é ç®—é‡‘é¡", "")
                é ç®—é‡‘é¡_æ•¸å­— = self.parse_budget_amount(é ç®—é‡‘é¡_åŸå§‹)
                    
                final_data = {
                    "é …æ¬¡": row_dict.get("é …æ¬¡", ""),
                    "æ©Ÿé—œåç¨±": row_dict.get("æ©Ÿé—œåç¨±", ""),
                    "æ¨™æ¡ˆç·¨è™Ÿ": ç·¨è™Ÿ,
                    "æ¨™æ¡ˆåç¨±": åç¨±,
                    "å‚³è¼¸æ¬¡æ•¸": row_dict.get("å‚³è¼¸æ¬¡æ•¸", ""),
                    "æ‹›æ¨™æ–¹å¼": row_dict.get("æ‹›æ¨™æ–¹å¼", ""),
                    "æ¡è³¼æ€§è³ª": row_dict.get("æ¡è³¼æ€§è³ª", ""),
                    "å…¬å‘Šæ—¥æœŸ": row_dict.get("å…¬å‘Šæ—¥æœŸ", ""),
                    "æˆªæ­¢æŠ•æ¨™": row_dict.get("æˆªæ­¢æŠ•æ¨™", ""),
                    "é ç®—é‡‘é¡": é ç®—é‡‘é¡_åŸå§‹,
                    "é ç®—é‡‘é¡_æ•¸å­—": é ç®—é‡‘é¡_æ•¸å­—,
                    "ç¶²å€": href_url,
                    "çˆ¬å–æ—¥æœŸ": today,
                    "é—œéµå­—": keyword
                }

                if åç¨± and row_dict.get("æ©Ÿé—œåç¨±"):
                    results.append(final_data)

            except Exception as e:
                logger.error(f"è§£æè¡Œè³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                continue

        logger.info(f"é—œéµå­— {keyword} ç²å¾— {len(results)} ç­†è³‡æ–™")
        return results
    
    def parse_budget_amount(self, budget_str: str) -> Optional[int]:
        """å°‡é ç®—é‡‘é¡å­—ä¸²è½‰æ›ç‚ºæ•¸å­—"""
        if not budget_str or budget_str == "æœªå®š":
            return None
        try:
            # ç§»é™¤é€—è™Ÿå’Œç©ºæ ¼ï¼Œæå–æ•¸å­—
            clean_str = budget_str.replace(",", "").replace(" ", "")
            # å°‹æ‰¾æ•¸å­—éƒ¨åˆ†
            import re
            numbers = re.findall(r'\d+', clean_str)
            if numbers:
                return int(numbers[0])
        except:
            pass
        return None
    
    def apply_filters(self, results: List[Dict], filters: Dict) -> List[Dict]:
        """å¥—ç”¨éæ¿¾æ¢ä»¶"""
        filtered_results = results
        
        # æ‹›æ¨™æ–¹å¼éæ¿¾
        if filters.get('tender_type'):
            tender_type = filters['tender_type']
            filtered_results = [r for r in filtered_results if tender_type in r.get('æ‹›æ¨™æ–¹å¼', '')]
        
        # æœ€ä½é ç®—éæ¿¾
        if filters.get('min_budget'):
            min_budget = filters['min_budget']
            filtered_results = [r for r in filtered_results if 
                              r.get('é ç®—é‡‘é¡_æ•¸å­—') and r['é ç®—é‡‘é¡_æ•¸å­—'] >= min_budget]
        
        # æ©Ÿé—œåç¨±éæ¿¾
        if filters.get('agency_filter'):
            agency_keyword = filters['agency_filter']
            filtered_results = [r for r in filtered_results if 
                              agency_keyword in r.get('æ©Ÿé—œåç¨±', '')]
        
        return filtered_results
    
    async def scrape_multiple_keywords(self, keywords: List[str], start_date: str, end_date: str, 
                                     page_size: int = 100, filters: Dict = None) -> List[Dict]:
        all_results = []
        for i, keyword in enumerate(keywords):
            if i > 0:  # ç¬¬ä¸€å€‹é—œéµå­—ä¸éœ€è¦ç­‰å¾…
                await asyncio.sleep(2)  # é¿å…å°ä¼ºæœå™¨é€ æˆéå¤§è² æ“”
            keyword_results = await self.scrape_by_keyword(keyword, start_date, end_date, page_size)
            all_results.extend(keyword_results)
        
        # å¥—ç”¨éæ¿¾æ¢ä»¶
        if filters:
            all_results = self.apply_filters(all_results, filters)
        
        # å»é™¤é‡è¤‡çš„æ¨™æ¡ˆï¼ˆæ ¹æ“šæ¨™æ¡ˆç·¨è™Ÿï¼‰
        seen_numbers = set()
        unique_results = []
        for result in all_results:
            tender_number = result.get('æ¨™æ¡ˆç·¨è™Ÿ', '')
            if tender_number and tender_number not in seen_numbers:
                seen_numbers.add(tender_number)
                unique_results.append(result)
            elif not tender_number:  # æ²’æœ‰ç·¨è™Ÿçš„ä¹Ÿä¿ç•™
                unique_results.append(result)
        
        return unique_results

scraper = PCCWebScraper()

@app.on_event("startup")
async def startup_event():
    await scraper.init_session()
    logger.info("æ”¿åºœæ¡è³¼ç¶²çˆ¬èŸ² API æœå‹™å·²å•Ÿå‹•")

@app.on_event("shutdown")
async def shutdown_event():
    await scraper.close_session()
    logger.info("æ”¿åºœæ¡è³¼ç¶²çˆ¬èŸ² API æœå‹™å·²é—œé–‰")

@app.get("/", response_class=HTMLResponse)
async def root():
    """API é¦–é  - æä¾›äº’å‹•å¼æ–‡ä»¶"""
    html_content = """
    <!DOCTYPE html>
    <html>
    <head>
        <title>æ”¿åºœæ¡è³¼ç¶²çˆ¬èŸ² API</title>
        <meta charset="utf-8">
        <style>
            body { font-family: Arial, sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; }
            .header { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
                     color: white; padding: 20px; border-radius: 10px; margin-bottom: 20px; }
            .api-info { background: #f8f9fa; padding: 15px; border-radius: 8px; margin-bottom: 15px; }
            .contact { background: #e3f2fd; padding: 15px; border-radius: 8px; }
            a { color: #1976d2; text-decoration: none; }
            a:hover { text-decoration: underline; }
        </style>
    </head>
    <body>
        <div class="header">
            <h1>ğŸš€ æ”¿åºœæ¡è³¼ç¶²çˆ¬èŸ² API</h1>
            <p>ç‰ˆæœ¬: 1.1.0 | æœå‹™ç‹€æ…‹: é‹è¡Œä¸­</p>
        </div>
        
        <div class="api-info">
            <h2>ğŸ“‹ API ç«¯é»</h2>
            <ul>
                <li><strong>GET /docs</strong> - Swagger UI äº’å‹•å¼æ–‡ä»¶</li>
                <li><strong>GET /redoc</strong> - ReDoc æ–‡ä»¶</li>
                <li><strong>GET /health</strong> - å¥åº·ç‹€æ…‹æª¢æŸ¥</li>
                <li><strong>POST /scrape</strong> - éˆæ´»çˆ¬èŸ²æœå°‹ (æ¨è–¦)</li>
                <li><strong>POST /scrape-today</strong> - ä»Šæ—¥å¿«é€Ÿæœå°‹</li>
                <li><strong>GET /api-status</strong> - è©³ç´°æœå‹™ç‹€æ…‹</li>
            </ul>
        </div>
        
        <div class="contact">
            <h2>ğŸ‘¨â€ğŸ’» é–‹ç™¼è€…è³‡è¨Š</h2>
            <p><strong>Nick Chang</strong><br>
            ğŸ“§ nickleo051216@gmail.com<br>
            ğŸ“± 0932-684-051<br>
            ğŸŒ <a href="https://portaly.cc/zn.studio">ZN Studio</a><br>
            ğŸ“± <a href="https://www.threads.com/@nickai216">Threads: @nickai216</a><br>
            ğŸ’¬ <a href="https://reurl.cc/1OZNAY">Line ç¤¾ç¾¤</a><br>
            ğŸ¤– <a href="https://lin.ee/Faz0doj">Line OA</a></p>
        </div>
    </body>
    </html>
    """
    return html_content

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """å¥åº·ç‹€æ…‹æª¢æŸ¥"""
    return HealthResponse(
        status="healthy",
        timestamp=datetime.datetime.now().isoformat(),
        service="æ”¿åºœæ¡è³¼ç¶²çˆ¬èŸ²",
        version="1.1.0",
        uptime_seconds=scraper.get_uptime()
    )

@app.get("/api-status")
async def api_status():
    """è©³ç´°çš„ API ç‹€æ…‹è³‡è¨Š"""
    return {
        "service": "æ”¿åºœæ¡è³¼ç¶²çˆ¬èŸ² API",
        "version": "1.1.0",
        "status": "running",
        "uptime_seconds": scraper.get_uptime(),
        "endpoints": {
            "scrape": "POST /scrape - éˆæ´»æœå°‹ (æ”¯æ´å¤šç¨®éæ¿¾æ¢ä»¶)",
            "scrape_today": "POST /scrape-today - ä»Šæ—¥å¿«é€Ÿæœå°‹",
            "health": "GET /health - å¥åº·æª¢æŸ¥",
            "docs": "GET /docs - Swagger æ–‡ä»¶"
        },
        "features": [
            "å¤šé—œéµå­—æœå°‹",
            "æ—¥æœŸç¯„åœéæ¿¾",
            "é ç®—é‡‘é¡éæ¿¾",
            "æ‹›æ¨™æ–¹å¼éæ¿¾",
            "æ©Ÿé—œåç¨±éæ¿¾",
            "é‡è¤‡è³‡æ–™è‡ªå‹•å»é™¤",
            "n8n ç›¸å®¹æ ¼å¼è¼¸å‡º"
        ],
        "author": "Nick Changï½œnickleo051216@gmail.comï½œ0932-684-051",
        "contact": {
            "website": "https://portaly.cc/zn.studio",
            "threads": "https://www.threads.com/@nickai216",
            "line_community": "https://reurl.cc/1OZNAY",
            "line_oa": "https://lin.ee/Faz0doj"
        },
        "timestamp": datetime.datetime.now().isoformat()
    }

@app.post("/scrape", response_model=ScrapeResponse)
async def scrape_tenders(request: ScrapeRequest):
    """
    éˆæ´»çš„æ”¿åºœæ¡è³¼ç¶²çˆ¬èŸ²
    
    æ”¯æ´å¤šç¨®æœå°‹æ¢ä»¶å’Œéæ¿¾é¸é …ï¼š
    - è‡ªè¨‚é—œéµå­—åˆ—è¡¨
    - æŒ‡å®šæ—¥æœŸç¯„åœ
    - é ç®—é‡‘é¡éæ¿¾
    - æ‹›æ¨™æ–¹å¼éæ¿¾
    - æ©Ÿé—œåç¨±éæ¿¾
    """
    # è¨­å®šé è¨­æ—¥æœŸ
    if not request.start_date:
        request.start_date = datetime.date.today().strftime('%Y/%m/%d')
    if not request.end_date:
        request.end_date = datetime.date.today().strftime('%Y/%m/%d')
    
    # æº–å‚™éæ¿¾æ¢ä»¶
    filters = {}
    if request.tender_type:
        filters['tender_type'] = request.tender_type
    if request.min_budget:
        filters['min_budget'] = request.min_budget
    if request.agency_filter:
        filters['agency_filter'] = request.agency_filter
    
    try:
        results = await scraper.scrape_multiple_keywords(
            keywords=request.search_terms,
            start_date=request.start_date,
            end_date=request.end_date,
            page_size=request.page_size,
            filters=filters if filters else None
        )
        
        # n8n ç›¸å®¹æ ¼å¼ - æ¯ç­†è³‡æ–™åŒ…è£åœ¨ json ç‰©ä»¶ä¸­
        n8n_format_results = [{"json": item} for item in results]
        
        return ScrapeResponse(
            success=True,
            data=n8n_format_results,
            count=len(results),
            message=f"æˆåŠŸçˆ¬å– {len(results)} ç­†è³‡æ–™",
            timestamp=datetime.datetime.now().isoformat(),
            filters_applied={
                "keywords": request.search_terms,
                "date_range": f"{request.start_date} ~ {request.end_date}",
                "page_size": request.page_size,
                **filters
            }
        )
        
    except Exception as e:
        logger.error(f"çˆ¬èŸ²åŸ·è¡ŒéŒ¯èª¤: {str(e)}")
        raise HTTPException(status_code=500, detail=f"çˆ¬èŸ²åŸ·è¡Œå¤±æ•—: {str(e)}")

@app.post("/scrape-today")
async def scrape_today(
    keywords: Optional[str] = Query(default="ç’°å¢ƒç›£æ¸¬,åœŸå£¤,åœ°ä¸‹æ°´,ç’°å¢ƒ", description="é—œéµå­—ï¼Œç”¨é€—è™Ÿåˆ†éš”"),
    page_size: Optional[int] = Query(default=100, description="æ¯é ç­†æ•¸")
):
    """
    ä»Šæ—¥å¿«é€Ÿçˆ¬èŸ² - ç›¸å®¹èˆŠç‰ˆ API
    
    ä¿æŒåŸæœ‰çš„å¿«é€Ÿæœå°‹åŠŸèƒ½ï¼Œä½†åŠ å…¥äº†åƒæ•¸åŒ–é¸é …
    """
    today = datetime.date.today().strftime('%Y/%m/%d')
    keyword_list = [k.strip() for k in keywords.split(',')]
    
    try:
        results = await scraper.scrape_multiple_keywords(
            keywords=keyword_list,
            start_date=today,
            end_date=today,
            page_size=page_size
        )
        
        # ä¿æŒåŸæœ‰çš„å›å‚³æ ¼å¼
        n8n_format_results = [{"json": item} for item in results]
        return n8n_format_results
        
    except Exception as e:
        logger.error(f"ä»Šæ—¥çˆ¬èŸ²åŸ·è¡ŒéŒ¯èª¤: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ä»Šæ—¥çˆ¬èŸ²åŸ·è¡Œå¤±æ•—: {str(e)}")

# æ–°å¢æ‰¹æ¬¡æŸ¥è©¢ç«¯é»
@app.post("/scrape-batch")
async def scrape_batch(requests: List[ScrapeRequest]):
    """
    æ‰¹æ¬¡çˆ¬èŸ²æŸ¥è©¢
    
    å¯ä»¥ä¸€æ¬¡åŸ·è¡Œå¤šå€‹ä¸åŒæ¢ä»¶çš„æœå°‹
    """
    if len(requests) > 5:  # é™åˆ¶æ‰¹æ¬¡æ•¸é‡
        raise HTTPException(status_code=400, detail="æ‰¹æ¬¡æŸ¥è©¢æœ€å¤šæ”¯æ´ 5 å€‹è«‹æ±‚")
    
    all_results = []
    for i, req in enumerate(requests):
        logger.info(f"åŸ·è¡Œæ‰¹æ¬¡æŸ¥è©¢ {i+1}/{len(requests)}")
        
        if not req.start_date:
            req.start_date = datetime.date.today().strftime('%Y/%m/%d')
        if not req.end_date:
            req.end_date = datetime.date.today().strftime('%Y/%m/%d')
            
        filters = {}
        if req.tender_type:
            filters['tender_type'] = req.tender_type
        if req.min_budget:
            filters['min_budget'] = req.min_budget
        if req.agency_filter:
            filters['agency_filter'] = req.agency_filter
        
        try:
            batch_results = await scraper.scrape_multiple_keywords(
                keywords=req.search_terms,
                start_date=req.start_date,
                end_date=req.end_date,
                page_size=req.page_size,
                filters=filters if filters else None
            )
            
            # æ¨™è¨˜æ‰¹æ¬¡ä¾†æº
            for result in batch_results:
                result['batch_index'] = i + 1
                result['batch_keywords'] = req.search_terms
                
            all_results.extend(batch_results)
            
            # æ‰¹æ¬¡é–“çš„ç­‰å¾…æ™‚é–“
            if i < len(requests) - 1:
                await asyncio.sleep(3)
                
        except Exception as e:
            logger.error(f"æ‰¹æ¬¡æŸ¥è©¢ {i+1} åŸ·è¡ŒéŒ¯èª¤: {str(e)}")
            continue
    
    n8n_format_results = [{"json": item} for item in all_results]
    
    return {
        "success": True,
        "data": n8n_format_results,
        "total_count": len(all_results),
        "batch_count": len(requests),
        "message": f"æ‰¹æ¬¡æŸ¥è©¢å®Œæˆï¼Œå…±ç²å¾— {len(all_results)} ç­†è³‡æ–™",
        "timestamp": datetime.datetime.now().isoformat()
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=False, log_level="info")
