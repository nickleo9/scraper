from fastapi import FastAPI, HTTPException, Query
from fastapi.responses import HTMLResponse
from pydantic import BaseModel, Field, validator
from typing import List, Optional, Dict, Union
import asyncio
import aiohttp
from bs4 import BeautifulSoup
import datetime
import json
import logging
from urllib.parse import urlencode, quote
import re

# è¨­å®šæ—¥èªŒè¨˜éŒ„
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å»ºç«‹ FastAPI æ‡‰ç”¨ç¨‹å¼
app = FastAPI(
    title="æ”¿åºœæ¡è³¼ç¶²çˆ¬èŸ² API",
    description="æä¾›æ”¿åºœæ¡è³¼ç¶²è³‡æ–™çˆ¬å–æœå‹™ - ZN Studio è£½ä½œ",
    version="1.2.0",
    contact={
        "name": "Nick Chang",
        "email": "nickleo051216@gmail.com",
        "url": "https://portaly.cc/zn.studio"
    },
    license_info={
        "name": "MIT",
        "url": "https://opensource.org/licenses/MIT"
    }
)

class ScrapeRequest(BaseModel):
    search_terms: List[str] = Field(
        default=["ç³»çµ±", "å¹³å°", "å»ºç½®", "ç¶­é‹"], 
        description="æœå°‹é—œéµå­—åˆ—è¡¨",
        example=["ç³»çµ±", "å¹³å°", "å»ºç½®"]
    )
    start_date: Optional[str] = Field(
        default=None, 
        description="é–‹å§‹æ—¥æœŸ (YYYY/MM/DD æ ¼å¼)",
        example="2024/09/01"
    )
    end_date: Optional[str] = Field(
        default=None, 
        description="çµæŸæ—¥æœŸ (YYYY/MM/DD æ ¼å¼)",
        example="2024/09/24"
    )
    page_size: int = Field(
        default=100, 
        ge=1, 
        le=1000, 
        description="æ¯é ç­†æ•¸ (1-1000)",
        example=100
    )
    tender_type: Optional[str] = Field(
        default=None,
        description="æ‹›æ¨™æ–¹å¼éæ¿¾ (å…¬é–‹æ‹›æ¨™ã€é™åˆ¶æ€§æ‹›æ¨™ç­‰)",
        example="å…¬é–‹æ‹›æ¨™"
    )
    min_budget: Optional[int] = Field(
        default=None,
        description="æœ€ä½é ç®—é‡‘é¡éæ¿¾",
        example=1000000
    )
    agency_filter: Optional[str] = Field(
        default=None,
        description="æ©Ÿé—œåç¨±éæ¿¾é—œéµå­—",
        example="ç’°ä¿ç½²"
    )

    @validator('start_date', 'end_date')
    def validate_date_format(cls, v):
        if v is None:
            return v
        try:
            datetime.datetime.strptime(v, '%Y/%m/%d')
            return v
        except ValueError:
            raise ValueError('æ—¥æœŸæ ¼å¼å¿…é ˆç‚º YYYY/MM/DD')

class ScrapeResponse(BaseModel):
    success: bool = Field(description="æ˜¯å¦æˆåŠŸ")
    data: List[Dict] = Field(description="çˆ¬å–è³‡æ–™")
    count: int = Field(description="è³‡æ–™ç­†æ•¸")
    message: str = Field(description="å›æ‡‰è¨Šæ¯")
    timestamp: str = Field(description="æ™‚é–“æˆ³è¨˜")
    filters_applied: Dict = Field(description="å¥—ç”¨çš„éæ¿¾æ¢ä»¶")

class HealthResponse(BaseModel):
    status: str
    timestamp: str
    service: str
    version: str
    uptime_seconds: Optional[float] = None

class PCCWebScraper:
    """æ”¿åºœæ¡è³¼ç¶²çˆ¬èŸ²é¡åˆ¥ - ä¿®æ­£ç‰ˆæœ¬"""
    
    def __init__(self):
        self.base_url = "https://web.pcc.gov.tw/prkms/tender/common/basic/readTenderBasic"
        self.session = None
        self.start_time = datetime.datetime.now()
        
    async def init_session(self):
        if not self.session:
            connector = aiohttp.TCPConnector(limit=10, ssl=False)
            timeout = aiohttp.ClientTimeout(total=30)
            self.session = aiohttp.ClientSession(
                connector=connector,
                timeout=timeout,
                headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'Connection': 'keep-alive'
                }
            )
    
    async def close_session(self):
        if self.session:
            await self.session.close()
            self.session = None
    
    def get_uptime(self) -> float:
        return (datetime.datetime.now() - self.start_time).total_seconds()
    
    async def scrape_by_keyword(self, keyword: str, start_date: str, end_date: str, page_size: int = 100) -> List[Dict]:
        if not self.session:
            await self.init_session()
            
        params = {
            'pageSize': page_size,
            'tenderStartDate': start_date,
            'tenderEndDate': end_date,
            'tenderName': keyword,
            'dateType': 'isDate'
        }
        query_string = urlencode(params, quote_via=quote)
        full_url = f"{self.base_url}?{query_string}"
        
        logger.info(f"æ­£åœ¨çˆ¬å–é—œéµå­—: {keyword} | æ—¥æœŸç¯„åœ: {start_date} - {end_date}")
        
        try:
            async with self.session.get(full_url) as response:
                if response.status != 200:
                    logger.error(f"è«‹æ±‚å¤±æ•—: {response.status}")
                    return []
                # ç¢ºä¿æ­£ç¢ºçš„ç·¨ç¢¼è™•ç†
                html_content = await response.text(encoding='utf-8')
                return self.parse_html_content(html_content, keyword)
        except Exception as e:
            logger.error(f"çˆ¬å–é—œéµå­— {keyword} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return []
    
    def clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡å­—ï¼Œç§»é™¤å¤šé¤˜ç©ºç™½å’Œæ›è¡Œ"""
        if not text:
            return ""
        # å…ˆæ­£è¦åŒ–ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text.strip())
        return text
    
    def parse_tender_info(self, cell_content: str) -> tuple:
        """æ›´ç²¾ç¢ºåœ°è§£ææ¨™æ¡ˆç·¨è™Ÿå’Œåç¨±"""
        if not cell_content:
            return "", ""
        
        # ä½¿ç”¨æ­£è¦è¡¨é”å¼ä¾†æ›´ç²¾ç¢ºåœ°åˆ†é›¢ç·¨è™Ÿå’Œåç¨±
        lines = [line.strip() for line in cell_content.split('\n') if line.strip()]
        
        if len(lines) == 0:
            return "", ""
        elif len(lines) == 1:
            # åªæœ‰ä¸€è¡Œçš„æƒ…æ³ï¼Œå¯èƒ½æ˜¯ç´”ç·¨è™Ÿæˆ–ç´”åç¨±
            single_line = lines[0]
            # æª¢æŸ¥æ˜¯å¦åƒç·¨è™Ÿæ ¼å¼ï¼ˆåŒ…å«æ•¸å­—å’Œè‹±æ–‡ï¼‰
            if re.match(r'^[A-Z0-9]+[-]?\d*$', single_line) or len(single_line) < 20:
                return single_line, ""  # ç•¶ä½œç·¨è™Ÿ
            else:
                return "", single_line  # ç•¶ä½œåç¨±
        else:
            # å¤šè¡Œçš„æƒ…æ³
            first_line = lines[0]
            rest_lines = ' '.join(lines[1:])
            
            # ç¬¬ä¸€è¡Œé€šå¸¸æ˜¯ç·¨è™Ÿï¼Œä½†è¦æª¢é©—
            if re.search(r'\d', first_line) and len(first_line) < 30:
                ç·¨è™Ÿ = first_line
                åç¨± = rest_lines
            else:
                # ç¬¬ä¸€è¡Œä¸åƒç·¨è™Ÿï¼Œå¯èƒ½æ•´å€‹éƒ½æ˜¯åç¨±
                ç·¨è™Ÿ = ""
                åç¨± = ' '.join(lines)
            
            return ç·¨è™Ÿ, åç¨±
    
    def parse_html_content(self, html: str, keyword: str) -> List[Dict]:
        """ä¿®æ­£å¾Œçš„HTMLå…§å®¹è§£æ"""
        soup = BeautifulSoup(html, 'html.parser')
        results = []
        table = soup.find('table', {'id': 'tpam'}) or soup.find('table', {'class': 'tb_01'})
        if not table:
            logger.warning(f"é—œéµå­— {keyword} æœªæ‰¾åˆ°è³‡æ–™è¡¨æ ¼")
            return []
        
        rows = table.find_all('tr')
        today = datetime.date.today().strftime('%Y/%m/%d')
        
        for row in rows[1:]:  # è·³éè¡¨é ­
            cols = row.find_all('td')
            if len(cols) < 9:
                continue
            try:
                # æ›´ä»”ç´°åœ°è™•ç†æ¯å€‹æ¬„ä½
                é …æ¬¡ = self.clean_text(cols[0].get_text())
                æ©Ÿé—œåç¨± = self.clean_text(cols[1].get_text())
                
                # é‡é»ä¿®æ­£ï¼šæ¨™æ¡ˆç·¨è™Ÿå’Œåç¨±çš„è§£æ
                æ¨™æ¡ˆè³‡è¨Š_åŸå§‹ = cols[2].get_text('\n').strip()
                æ¨™æ¡ˆç·¨è™Ÿ, æ¨™æ¡ˆåç¨± = self.parse_tender_info(æ¨™æ¡ˆè³‡è¨Š_åŸå§‹)
                
                å‚³è¼¸æ¬¡æ•¸ = self.clean_text(cols[3].get_text())
                æ‹›æ¨™æ–¹å¼ = self.clean_text(cols[4].get_text())
                æ¡è³¼æ€§è³ª = self.clean_text(cols[5].get_text())
                å…¬å‘Šæ—¥æœŸ = self.clean_text(cols[6].get_text())
                æˆªæ­¢æŠ•æ¨™ = self.clean_text(cols[7].get_text())
                é ç®—é‡‘é¡ = self.clean_text(cols[8].get_text())

                # å–å¾—é€£çµ
                href_url = ""
                a_tag = cols[2].find("a")
                if a_tag and a_tag.get('href') and 'pk=' in a_tag.get('href'):
                    pk_val = a_tag['href'].split('pk=')[-1].split('&')[0]
                    href_url = f"https://web.pcc.gov.tw/tps/QueryTender/query/searchTenderDetail?pkPmsMain={pk_val}"
                
                # è™•ç†é ç®—é‡‘é¡ï¼Œè½‰æ›ç‚ºæ•¸å­—ä¾¿æ–¼éæ¿¾
                é ç®—é‡‘é¡_æ•¸å­— = self.parse_budget_amount(é ç®—é‡‘é¡)
                    
                final_data = {
                    "é …æ¬¡": é …æ¬¡,
                    "æ©Ÿé—œåç¨±": æ©Ÿé—œåç¨±,
                    "æ¨™æ¡ˆç·¨è™Ÿ": æ¨™æ¡ˆç·¨è™Ÿ,
                    "æ¨™æ¡ˆåç¨±": æ¨™æ¡ˆåç¨±,
                    "å‚³è¼¸æ¬¡æ•¸": å‚³è¼¸æ¬¡æ•¸,
                    "æ‹›æ¨™æ–¹å¼": æ‹›æ¨™æ–¹å¼,
                    "æ¡è³¼æ€§è³ª": æ¡è³¼æ€§è³ª,
                    "å…¬å‘Šæ—¥æœŸ": å…¬å‘Šæ—¥æœŸ,
                    "æˆªæ­¢æŠ•æ¨™": æˆªæ­¢æŠ•æ¨™,
                    "é ç®—é‡‘é¡": é ç®—é‡‘é¡,
                    "é ç®—é‡‘é¡_æ•¸å­—": é ç®—é‡‘é¡_æ•¸å­—,
                    "ç¶²å€": href_url,
                    "çˆ¬å–æ—¥æœŸ": today,
                    "é—œéµå­—": keyword,
                    "åŸå§‹æ¨™æ¡ˆè³‡è¨Š": æ¨™æ¡ˆè³‡è¨Š_åŸå§‹  # é™¤éŒ¯ç”¨ï¼Œå¯ä»¥ç§»é™¤
                }

                # åªæœ‰ç•¶æ©Ÿé—œåç¨±å’Œæ¨™æ¡ˆåç¨±éƒ½æœ‰å…§å®¹æ™‚æ‰åŠ å…¥
                if æ©Ÿé—œåç¨± and (æ¨™æ¡ˆåç¨± or æ¨™æ¡ˆç·¨è™Ÿ):
                    results.append(final_data)
                else:
                    logger.debug(f"è·³éç„¡æ•ˆè³‡æ–™: æ©Ÿé—œ={æ©Ÿé—œåç¨±}, ç·¨è™Ÿ={æ¨™æ¡ˆç·¨è™Ÿ}, åç¨±={æ¨™æ¡ˆåç¨±}")

            except Exception as e:
                logger.error(f"è§£æè¡Œè³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                logger.debug(f"å•é¡Œè¡Œå…§å®¹: {[col.get_text() for col in cols[:3]]}")
                continue

        logger.info(f"é—œéµå­— {keyword} ç²å¾— {len(results)} ç­†æœ‰æ•ˆè³‡æ–™")
        return results
    
    def parse_budget_amount(self, budget_str: str) -> Optional[int]:
        """å°‡é ç®—é‡‘é¡å­—ä¸²è½‰æ›ç‚ºæ•¸å­—"""
        if not budget_str or budget_str == "æœªå®š":
            return None
        try:
            # ç§»é™¤é€—è™Ÿå’Œç©ºæ ¼ï¼Œæå–æ•¸å­—
            clean_str = budget_str.replace(",", "").replace(" ", "")
            # å°‹æ‰¾æ•¸å­—éƒ¨åˆ†
            numbers = re.findall(r'\d+', clean_str)
            if numbers:
                return int(numbers[0])
        except:
            pass
        return None
    
    def apply_filters(self, results: List[Dict], filters: Dict) -> List[Dict]:
        """å¥—ç”¨éæ¿¾æ¢ä»¶"""
        filtered_results = results
        
        # æ‹›æ¨™æ–¹å¼éæ¿¾
        if filters.get('tender_type'):
            tender_type = filters['tender_type']
            filtered_results = [r for r in filtered_results if tender_type in r.get('æ‹›æ¨™æ–¹å¼', '')]
        
        # æœ€ä½é ç®—éæ¿¾
        if filters.get('min_budget'):
            min_budget = filters['min_budget']
            filtered_results = [r for r in filtered_results if 
                              r.get('é ç®—é‡‘é¡_æ•¸å­—') and r['é ç®—é‡‘é¡_æ•¸å­—'] >= min_budget]
        
        # æ©Ÿé—œåç¨±éæ¿¾
        if filters.get('agency_filter'):
            agency_keyword = filters['agency_filter']
            filtered_results = [r for r in filtered_results if 
                              agency_keyword in r.get('æ©Ÿé—œåç¨±', '')]
        
        return filtered_results
    
    async def scrape_multiple_keywords(self, keywords: List[str], start_date: str, end_date: str, 
                                     page_size: int = 100, filters: Dict = None) -> List[Dict]:
        all_results = []
        for i, keyword in enumerate(keywords):
            if i > 0:  # ç¬¬ä¸€å€‹é—œéµå­—ä¸éœ€è¦ç­‰å¾…
                await asyncio.sleep(2)  # é¿å…å°ä¼ºæœå™¨é€ æˆéå¤§è² æ“”
            keyword_results = await self.scrape_by_keyword(keyword, start_date, end_date, page_size)
            all_results.extend(keyword_results)
        
        # å¥—ç”¨éæ¿¾æ¢ä»¶
        if filters:
            all_results = self.apply_filters(all_results, filters)
        
        # å»é™¤é‡è¤‡çš„æ¨™æ¡ˆï¼ˆæ ¹æ“šæ¨™æ¡ˆç·¨è™Ÿå’Œæ©Ÿé—œåç¨±ï¼‰
        seen_combinations = set()
        unique_results = []
        for result in all_results:
            identifier = f"{result.get('æ©Ÿé—œåç¨±', '')}_{result.get('æ¨™æ¡ˆç·¨è™Ÿ', '')}_{result.get('æ¨™æ¡ˆåç¨±', '')}"
            if identifier not in seen_combinations:
                seen_combinations.add(identifier)
                unique_results.append(result)
        
        return unique_results

scraper = PCCWebScraper()

@app.on_event("startup")
async def startup_event():
    await scraper.init_session()
    logger.info("æ”¿åºœæ¡è³¼ç¶²çˆ¬èŸ² API æœå‹™å·²å•Ÿå‹• - ä¿®æ­£ç‰ˆæœ¬ 1.2.0")

@app.on_event("shutdown")
async def shutdown_event():
    await scraper.close_session()
    logger.info("æ”¿åºœæ¡è³¼ç¶²çˆ¬èŸ² API æœå‹™å·²é—œé–‰")

@app.get("/", response_class=HTMLResponse)
async def root():
    """API é¦–é  - æä¾›äº’å‹•å¼æ–‡ä»¶"""
    html_content = """
    <!DOCTYPE html>
    <html>
    <head>
        <title>æ”¿åºœæ¡è³¼ç¶²çˆ¬èŸ² API v1.2</title>
        <meta charset="utf-8">
        <style>
            body { font-family: Arial, sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; }
            .header { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
                     color: white; padding: 20px; border-radius: 10px; margin-bottom: 20px; }
            .api-info { background: #f8f9fa; padding: 15px; border-radius: 8px; margin-bottom: 15px; }
            .contact { background: #e3f2fd; padding: 15px; border-radius: 8px; }
            .changelog { background: #fff3e0; padding: 15px; border-radius: 8px; margin-bottom: 15px; }
            a { color: #1976d2; text-decoration: none; }
            a:hover { text-decoration: underline; }
        </style>
    </head>
    <body>
        <div class="header">
            <h1>ğŸš€ æ”¿åºœæ¡è³¼ç¶²çˆ¬èŸ² API v1.2</h1>
            <p>ç‰ˆæœ¬: 1.2.0 | æœå‹™ç‹€æ…‹: é‹è¡Œä¸­ | ä¿®æ­£æ¨™æ¡ˆç·¨è™Ÿåç¨±è§£æå•é¡Œ</p>
        </div>
        
        <div class="changelog">
            <h2>ğŸ”§ v1.2.0 æ›´æ–°å…§å®¹</h2>
            <ul>
                <li>âœ… ä¿®æ­£æ¨™æ¡ˆç·¨è™Ÿå’Œæ¨™æ¡ˆåç¨±æ··æ·†å•é¡Œ</li>
                <li>âœ… æ”¹å–„ä¸­æ–‡å­—ç¬¦ç·¨ç¢¼è™•ç†</li>
                <li>âœ… å¼·åŒ–HTMLè§£æé‚è¼¯</li>
                <li>âœ… å¢åŠ æ›´ç²¾ç¢ºçš„è³‡æ–™é©—è­‰</li>
                <li>âœ… å„ªåŒ–é™¤éŒ¯è³‡è¨Šè¼¸å‡º</li>
            </ul>
        </div>
        
        <div class="api-info">
            <h2>ğŸ“‹ API ç«¯é»</h2>
            <ul>
                <li><strong>GET /docs</strong> - Swagger UI äº’å‹•å¼æ–‡ä»¶</li>
                <li><strong>GET /redoc</strong> - ReDoc æ–‡ä»¶</li>
                <li><strong>GET /health</strong> - å¥åº·ç‹€æ…‹æª¢æŸ¥</li>
                <li><strong>POST /scrape</strong> - éˆæ´»çˆ¬èŸ²æœå°‹ (æ¨è–¦)</li>
                <li><strong>POST /scrape-today</strong> - ä»Šæ—¥å¿«é€Ÿæœå°‹</li>
                <li><strong>GET /api-status</strong> - è©³ç´°æœå‹™ç‹€æ…‹</li>
            </ul>
        </div>
        
        <div class="contact">
            <h2>ğŸ‘¨â€ğŸ’» é–‹ç™¼è€…è³‡è¨Š</h2>
            <p><strong>Nick Chang</strong><br>
            ğŸ“§ nickleo051216@gmail.com<br>
            ğŸ“± 0932-684-051<br>
            ğŸŒ <a href="https://portaly.cc/zn.studio">ZN Studio</a><br>
            ğŸ“± <a href="https://www.threads.com/@nickai216">Threads: @nickai216</a><br>
            ğŸ’¬ <a href="https://reurl.cc/1OZNAY">Line ç¤¾ç¾¤</a><br>
            ğŸ¤– <a href="https://lin.ee/Faz0doj">Line OA</a></p>
        </div>
    </body>
    </html>
    """
    return html_content

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """å¥åº·ç‹€æ…‹æª¢æŸ¥"""
    return HealthResponse(
        status="healthy",
        timestamp=datetime.datetime.now().isoformat(),
        service="æ”¿åºœæ¡è³¼ç¶²çˆ¬èŸ²",
        version="1.2.0",
        uptime_seconds=scraper.get_uptime()
    )

@app.get("/api-status")
async def api_status():
    """è©³ç´°çš„ API ç‹€æ…‹è³‡è¨Š"""
    return {
        "service": "æ”¿åºœæ¡è³¼ç¶²çˆ¬èŸ² API",
        "version": "1.2.0",
        "status": "running",
        "uptime_seconds": scraper.get_uptime(),
        "changelog": {
            "v1.2.0": [
                "ä¿®æ­£æ¨™æ¡ˆç·¨è™Ÿå’Œæ¨™æ¡ˆåç¨±æ··æ·†å•é¡Œ",
                "æ”¹å–„ä¸­æ–‡å­—ç¬¦ç·¨ç¢¼è™•ç†",
                "å¼·åŒ–HTMLè§£æé‚è¼¯",
                "å¢åŠ æ›´ç²¾ç¢ºçš„è³‡æ–™é©—è­‰",
                "å„ªåŒ–é™¤éŒ¯è³‡è¨Šè¼¸å‡º"
            ]
        },
        "endpoints": {
            "scrape": "POST /scrape - éˆæ´»æœå°‹ (æ”¯æ´å¤šç¨®éæ¿¾æ¢ä»¶)",
            "scrape_today": "POST /scrape-today - ä»Šæ—¥å¿«é€Ÿæœå°‹",
            "health": "GET /health - å¥åº·æª¢æŸ¥",
            "docs": "GET /docs - Swagger æ–‡ä»¶"
        },
        "features": [
            "å¤šé—œéµå­—æœå°‹",
            "æ—¥æœŸç¯„åœéæ¿¾",
            "é ç®—é‡‘é¡éæ¿¾",
            "æ‹›æ¨™æ–¹å¼éæ¿¾",
            "æ©Ÿé—œåç¨±éæ¿¾",
            "é‡è¤‡è³‡æ–™è‡ªå‹•å»é™¤",
            "n8n ç›¸å®¹æ ¼å¼è¼¸å‡º",
            "ç²¾ç¢ºçš„æ¨™æ¡ˆç·¨è™Ÿåç¨±è§£æ"
        ],
        "author": "Nick Changï½œnickleo051216@gmail.comï½œ0932-684-051",
        "contact": {
            "website": "https://portaly.cc/zn.studio",
            "threads": "https://www.threads.com/@nickai216",
            "line_community": "https://reurl.cc/1OZNAY",
            "line_oa": "https://lin.ee/Faz0doj"
        },
        "timestamp": datetime.datetime.now().isoformat()
    }

@app.post("/scrape", response_model=ScrapeResponse)
async def scrape_tenders(request: ScrapeRequest):
    """
    éˆæ´»çš„æ”¿åºœæ¡è³¼ç¶²çˆ¬èŸ² - ä¿®æ­£ç‰ˆ
    
    v1.2.0 æ›´æ–°:
    - ä¿®æ­£æ¨™æ¡ˆç·¨è™Ÿå’Œæ¨™æ¡ˆåç¨±æ··æ·†å•é¡Œ
    - æ”¹å–„ä¸­æ–‡å­—ç¬¦ç·¨ç¢¼è™•ç†
    - å¼·åŒ–HTMLè§£æé‚è¼¯
    """
    # è¨­å®šé è¨­æ—¥æœŸ
    if not request.start_date:
        request.start_date = datetime.date.today().strftime('%Y/%m/%d')
    if not request.end_date:
        request.end_date = datetime.date.today().strftime('%Y/%m/%d')
    
    # æº–å‚™éæ¿¾æ¢ä»¶
    filters = {}
    if request.tender_type:
        filters['tender_type'] = request.tender_type
    if request.min_budget:
        filters['min_budget'] = request.min_budget
    if request.agency_filter:
        filters['agency_filter'] = request.agency_filter
    
    try:
        results = await scraper.scrape_multiple_keywords(
            keywords=request.search_terms,
            start_date=request.start_date,
            end_date=request.end_date,
            page_size=request.page_size,
            filters=filters if filters else None
        )
        
        # n8n ç›¸å®¹æ ¼å¼ - æ¯ç­†è³‡æ–™åŒ…è£åœ¨ json ç‰©ä»¶ä¸­
        n8n_format_results = [{"json": item} for item in results]
        
        return ScrapeResponse(
            success=True,
            data=n8n_format_results,
            count=len(results),
            message=f"æˆåŠŸçˆ¬å– {len(results)} ç­†è³‡æ–™ (v1.2.0ä¿®æ­£ç‰ˆ)",
            timestamp=datetime.datetime.now().isoformat(),
            filters_applied={
                "keywords": request.search_terms,
                "date_range": f"{request.start_date} ~ {request.end_date}",
                "page_size": request.page_size,
                **filters
            }
        )
        
    except Exception as e:
        logger.error(f"çˆ¬èŸ²åŸ·è¡ŒéŒ¯èª¤: {str(e)}")
        raise HTTPException(status_code=500, detail=f"çˆ¬èŸ²åŸ·è¡Œå¤±æ•—: {str(e)}")

@app.post("/scrape-today")
async def scrape_today(
    keywords: Optional[str] = Query(default="ç’°å¢ƒç›£æ¸¬,åœŸå£¤,åœ°ä¸‹æ°´,ç’°å¢ƒ", description="é—œéµå­—ï¼Œç”¨é€—è™Ÿåˆ†éš”"),
    page_size: Optional[int] = Query(default=100, description="æ¯é ç­†æ•¸")
):
    """
    ä»Šæ—¥å¿«é€Ÿçˆ¬èŸ² - ä¿®æ­£ç‰ˆæœ¬
    
    v1.2.0 æ›´æ–°: ä¿®æ­£æ¨™æ¡ˆç·¨è™Ÿå’Œæ¨™æ¡ˆåç¨±è§£æå•é¡Œ
    """
    today = datetime.date.today().strftime('%Y/%m/%d')
    keyword_list = [k.strip() for k in keywords.split(',')]
    
    try:
        results = await scraper.scrape_multiple_keywords(
            keywords=keyword_list,
            start_date=today,
            end_date=today,
            page_size=page_size
        )
        
        # ä¿æŒåŸæœ‰çš„å›å‚³æ ¼å¼
        n8n_format_results = [{"json": item} for item in results]
        return n8n_format_results
        
    except Exception as e:
        logger.error(f"ä»Šæ—¥çˆ¬èŸ²åŸ·è¡ŒéŒ¯èª¤: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ä»Šæ—¥çˆ¬èŸ²åŸ·è¡Œå¤±æ•—: {str(e)}")

# æ–°å¢æ¸¬è©¦ç«¯é»
@app.get("/test-parsing")
async def test_parsing():
    """æ¸¬è©¦HTMLè§£æåŠŸèƒ½"""
    test_html = """
    <td>
        114BB0013 (æ›´æ­£å…¬å‘Š)<br>
        æ¡ƒæºå€æ¢…å±±åœ°å€ç’°å¢ƒæ•´é«”ç‡Ÿé€ å·¥ç¨‹
    </td>
    """
    
    soup = BeautifulSoup(test_html, 'html.parser')
    cell_content = soup.get_text('\n').strip()
    ç·¨è™Ÿ, åç¨± = scraper.parse_tender_info(cell_content)
    
    return {
        "åŸå§‹å…§å®¹": cell_content,
        "è§£æçµæœ": {
            "æ¨™æ¡ˆç·¨è™Ÿ": ç·¨è™Ÿ,
            "æ¨™æ¡ˆåç¨±": åç¨±
        },
        "èªªæ˜": "é€™æ˜¯æ¸¬è©¦è§£æåŠŸèƒ½çš„ç«¯é»"
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=False, log_level="info")
